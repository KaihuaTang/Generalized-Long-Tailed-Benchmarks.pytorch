{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2\n",
    "#from sklearn.externals import joblib\n",
    "# Python 3\n",
    "import joblib\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(25)\n",
    "\n",
    "COCO_IMAGE_PATH = '/data4/coco2014/images/'\n",
    "COCO_ATTRIBUTE_PATH = './cocottributes_py3.jbl'\n",
    "COCO_ANNOTATION_PATH = '/data4/coco2014/annotations/'\n",
    "OUTPUT_PATH = './coco_intra_{}_inter_{}.jbl'   # lt (long-tailed) / bl (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco(root_path=COCO_ANNOTATION_PATH):\n",
    "    # Load COCO Annotations in val2014 & train2014\n",
    "    coco_data = {'images':[], 'annotations':[]}\n",
    "    with open(os.path.join(root_path, 'instances_train2014.json'), 'r') as f:\n",
    "        train2014 = json.load(f)\n",
    "    with open(os.path.join(root_path, 'instances_val2014.json'), 'r') as f:\n",
    "        val2014 = json.load(f)\n",
    "    coco_data['categories'] = train2014['categories']\n",
    "    coco_data['images'] += train2014['images']\n",
    "    coco_data['images'] += val2014['images']\n",
    "    coco_data['annotations'] += train2014['annotations']\n",
    "    coco_data['annotations'] += val2014['annotations']\n",
    "    return coco_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(cocottributes, ann_vecs, coco_data):\n",
    "    att_statistics = {}\n",
    "    cat_statistics = {}\n",
    "    key2cats = {}\n",
    "    attr_details = sorted(cocottributes['attributes'], key=lambda x:x['id'])\n",
    "    attr_names = [item['name'] for item in attr_details]\n",
    "    obj_id2annos = {ann['id'] : ann for ann in coco_data['annotations']}\n",
    "    cat_id2cats = {c['id'] : c['name'] for c in coco_data['categories']}\n",
    "    \n",
    "    for key in list(ann_vecs.keys()):\n",
    "        instance_attrs = ann_vecs[key]\n",
    "        # category statistics\n",
    "        coco_obj_id = cocottributes['patch_id_to_ann_id'][key]\n",
    "        coco_annotation = obj_id2annos[coco_obj_id]\n",
    "        cat = cat_id2cats[coco_annotation['category_id']]\n",
    "        key2cats[key] = cat\n",
    "        cat_statistics[cat] = cat_statistics.get(cat, 0) + 1\n",
    "\n",
    "        # attribute statistics\n",
    "        pos_attrs = [a for att_id, a in enumerate(attr_names) if instance_attrs[att_id] > 0.5]\n",
    "        for att in pos_attrs:\n",
    "            att_statistics[att] = att_statistics.get(att, 0) + 1\n",
    "\n",
    "    return att_statistics, cat_statistics, key2cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_val_test(cocottributes, key2cats, TEST_SIZE=100, VAL_SIZE=50, intra_type='lt', inter_type='lt'):\n",
    "    ann_vecs = cocottributes['ann_vecs']\n",
    "    # init data splits\n",
    "    trainset =         {'label':{}, 'frequency':{}, 'attribute':{}}\n",
    "    valset =           {'label':{}, 'frequency':{}, 'attribute':{}}\n",
    "    testset_lt =       {'label':{}, 'frequency':{}, 'attribute':{}}\n",
    "    testset_bl =       {'label':{}, 'frequency':{}, 'attribute':{}}\n",
    "    testset_bbl =      {'label':{}, 'frequency':{}, 'attribute':{}}\n",
    "    \n",
    "    NUM_CAT = len(cat_statistics)\n",
    "    NUM_ATT = len(att_statistics)\n",
    "    print('number of category: ', NUM_CAT)\n",
    "    print('number of attribute: ', NUM_ATT)\n",
    "    \n",
    "    # index keys by category\n",
    "    cat_keys = {cat : {} for cat in set(key2cats.values())}\n",
    "    for key, val in ann_vecs.items():\n",
    "        cat = key2cats[key]\n",
    "        cat_keys[cat][key] = val\n",
    "    # generate frequent label\n",
    "    CAT2FRQ, CAT2ID = generate_freq_label(cat_keys)\n",
    "    print('Data Size After {} is {}'.format('Init', len(ann_vecs)))\n",
    "    print('Data Size After {} is {}'.format('Init', sum([len(val) for key, val in cat_keys.items()])))\n",
    "    \n",
    "    #################################################\n",
    "    # TEST BBL: generate test set that has the balanced\n",
    "    # distribution for both category and attribute\n",
    "    #################################################\n",
    "    cat_keys, ann_vecs, testset_bbl = generate_balanced_test(cat_keys, ann_vecs, testset_bbl, TEST_SIZE, NUM_ATT, CAT2FRQ, CAT2ID)\n",
    "    print('Data Size After {} is {}'.format('Test-BBL', len(ann_vecs)))\n",
    "    print('Data Size After {} is {}'.format('Test-BBL', sum([len(val) for key, val in cat_keys.items()])))\n",
    "    \n",
    "    \n",
    "    #######################################################\n",
    "    # TEST BL: generate test set that only has the balanced\n",
    "    # class distribution\n",
    "    #######################################################\n",
    "    cat_keys, ann_vecs, testset_bl = generate_intra_lt_test(cat_keys, ann_vecs, testset_bl, TEST_SIZE, CAT2FRQ, CAT2ID)\n",
    "    print('Data Size After {} is {}'.format('Test-BL', len(ann_vecs)))\n",
    "    print('Data Size After {} is {}'.format('Test-BL', sum([len(val) for key, val in cat_keys.items()])))\n",
    "    \n",
    "    \n",
    "    #################################################\n",
    "    # Generate Imbalanced Dataset\n",
    "    #################################################\n",
    "    if intra_type == 'lt' and inter_type == 'lt':\n",
    "        trainset = generate_data(cat_keys, ann_vecs, trainset, CAT2FRQ, CAT2ID, imb_type='exp')\n",
    "    elif intra_type == 'lt' and inter_type == 'bl':\n",
    "        trainset = generate_data(cat_keys, ann_vecs, trainset, CAT2FRQ, CAT2ID, imb_type='bl')\n",
    "    else:\n",
    "        raise ValueError('Wrong Combination of Distribution Types')\n",
    "    print('Data Size After {} is {}'.format('Selection', len(trainset['label'])))\n",
    "    \n",
    "    \n",
    "    #################################################\n",
    "    # TEST LT: generate test set that has the same\n",
    "    # distribution with long-tailed train set\n",
    "    #################################################\n",
    "    if intra_type == 'lt' and inter_type == 'lt':\n",
    "        testset_lt = generate_iid_set(trainset, testset_lt, TEST_SIZE * NUM_CAT)\n",
    "    print('Data Size After {} is {}'.format('Selection', len(trainset['label'])))\n",
    "    \n",
    "    \n",
    "    #################################################\n",
    "    # VAL: generate validation set, the distribution \n",
    "    # of val set should be the same as train set\n",
    "    #################################################\n",
    "    valset = generate_iid_set(trainset, valset, VAL_SIZE * NUM_CAT)\n",
    "    print('Data Size After {} is {}'.format('Selection', len(trainset['label'])))\n",
    "    \n",
    "    return trainset, valset, testset_lt, testset_bl, testset_bbl, CAT2ID\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# useful functions\n",
    "\n",
    "def generate_freq_label(cat_keys):\n",
    "    cls_sizes_with_cats = [(len(val), key) for key, val in cat_keys.items()]\n",
    "    cls_sizes_with_cats.sort(key=lambda x: x[0], reverse=True)\n",
    "    print('Class Size with Cat: ', str(cls_sizes_with_cats))\n",
    "    # freq label and label index\n",
    "    cat2frq = {}\n",
    "    cat2id = {}\n",
    "    for i, item in enumerate(cls_sizes_with_cats):\n",
    "        # label index\n",
    "        cat2id[item[1]] = i\n",
    "        # freq label\n",
    "        if i <= 10:\n",
    "            cat2frq[item[1]] = 0\n",
    "        elif i <= 22:\n",
    "            cat2frq[item[1]] = 1\n",
    "        else:\n",
    "            cat2frq[item[1]] = 2\n",
    "    return cat2frq, cat2id\n",
    "\n",
    "\n",
    "def normalize_vector(vector):\n",
    "    output = vector / (vector.sum() + 1e-9)\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_balanced_test(cat_keys, ann_vecs, outputset, VAL_SIZE, NUM_ATT, CAT2FRQ, CAT2ID):\n",
    "    for i, (c_key, c_val) in enumerate(cat_keys.items()):\n",
    "        test_dist = np.array([0.0 for _ in range(NUM_ATT)])\n",
    "        print('===== Processing: {} ====='.format(i/len(cat_keys)))\n",
    "        cat_count = 0\n",
    "        while(cat_count<VAL_SIZE):\n",
    "            min_std = 999999999.9\n",
    "            min_key = None\n",
    "            min_val = None\n",
    "            for key, val in c_val.items():\n",
    "                val = (val > 0.5).astype(np.float32)\n",
    "                if val.sum() == 0:\n",
    "                    continue\n",
    "                tmp_dist = (test_dist + val)\n",
    "                if normalize_vector(tmp_dist).std() < min_std:\n",
    "                    min_std = normalize_vector(tmp_dist).std()\n",
    "                    min_key = key\n",
    "                    min_val = val\n",
    "            outputset['label'][min_key] = CAT2ID[c_key]\n",
    "            outputset['frequency'][min_key] = CAT2FRQ[c_key]\n",
    "            outputset['attribute'][min_key] = min_val\n",
    "            test_dist += min_val\n",
    "            cat_count += 1\n",
    "            del ann_vecs[min_key]\n",
    "            del c_val[min_key]\n",
    "    return cat_keys, ann_vecs, outputset\n",
    "\n",
    "\n",
    "\n",
    "def generate_intra_lt_test(cat_keys, ann_vecs, outputset, VAL_SIZE, CAT2FRQ, CAT2ID):\n",
    "    for c_key, c_val in cat_keys.items():\n",
    "        cat_count = 0\n",
    "        while(cat_count < VAL_SIZE):\n",
    "            idx = random.randint(0, len(c_val)-1)\n",
    "            key = list(c_val.keys())[idx]\n",
    "            outputset['label'][key] = CAT2ID[c_key]\n",
    "            outputset['frequency'][key] = CAT2FRQ[c_key]\n",
    "            outputset['attribute'][key] = c_val[key]\n",
    "            cat_count += 1\n",
    "            del ann_vecs[key]\n",
    "            del c_val[key]\n",
    "    return cat_keys, ann_vecs, outputset\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(cat_keys, ann_vecs, outputset, CAT2FRQ, CAT2ID, imb_type='bl', imb_factor=0.005):\n",
    "    cls_sizes_with_cats = [(len(val), key) for key, val in cat_keys.items()]\n",
    "    cls_sizes_with_cats.sort(key=lambda x: x[0], reverse=True)\n",
    "    cls_sizes = [item[0] for item in cls_sizes_with_cats]\n",
    "    max_size, min_size = max(cls_sizes), min(cls_sizes)\n",
    "    print('Max/min class sizes are {} / {}'.format(max_size, min_size))\n",
    "    cls_num = len(cls_sizes)\n",
    "    print('Num of class is {}'.format(cls_num))\n",
    "\n",
    "    img_num_per_cls = []\n",
    "    if imb_type == 'bl':\n",
    "        img_num_per_cls = [min_size] * cls_num\n",
    "    elif imb_type == 'exp':\n",
    "        for cls_idx in range(cls_num):\n",
    "            num = max_size * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "            img_num_per_cls.append(min(int(num), cls_sizes[cls_idx]))\n",
    "    else:\n",
    "        print('Wrong imbalance type')\n",
    "        \n",
    "    for i, item in enumerate(cls_sizes_with_cats):\n",
    "        c_key = item[1]\n",
    "        c_val = cat_keys[c_key]\n",
    "        cat_count = 0\n",
    "        THIS_SIZE = img_num_per_cls[i]\n",
    "        while(cat_count < THIS_SIZE):\n",
    "            idx = random.randint(0, len(c_val)-1)\n",
    "            key = list(c_val.keys())[idx]\n",
    "            outputset['label'][key] = CAT2ID[c_key]\n",
    "            outputset['frequency'][key] = CAT2FRQ[c_key]\n",
    "            outputset['attribute'][key] = c_val[key]\n",
    "            cat_count += 1\n",
    "            del ann_vecs[key]\n",
    "            del c_val[key]\n",
    "        \n",
    "    return outputset\n",
    "\n",
    "\n",
    "def generate_iid_set(inputset, outputset, TOTAL_SIZE):\n",
    "    total_count = 0\n",
    "    while(total_count < TOTAL_SIZE):\n",
    "        idx = random.randint(0, len(inputset['label'])-1)\n",
    "        key = list(inputset['label'].keys())[idx]\n",
    "        outputset['label'][key] = inputset['label'][key]\n",
    "        outputset['frequency'][key] = inputset['frequency'][key]\n",
    "        outputset['attribute'][key] = inputset['attribute'][key]\n",
    "        total_count += 1\n",
    "        # remove selected items\n",
    "        del inputset['label'][key]\n",
    "        del inputset['frequency'][key]\n",
    "        del inputset['attribute'][key]\n",
    "    return outputset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output():\n",
    "    output_dict = {}\n",
    "    output_dict.update(cocottributes)\n",
    "    \n",
    "    print('Train size is : {}'.format(len(trainset['label'])))\n",
    "    print('Val size is : {}'.format(len(valset['label'])))\n",
    "    print('Test-LT size is : {}'.format(len(testset_lt['label'])))\n",
    "    print('Test-BL size is : {}'.format(len(testset_bl['label'])))\n",
    "    print('Test-BBL size is : {}'.format(len(testset_bbl['label'])))\n",
    "    \n",
    "    output_dict['train'] = trainset\n",
    "    output_dict['val'] = valset\n",
    "    output_dict['test_lt'] = testset_lt\n",
    "    output_dict['test_bl'] = testset_bl\n",
    "    output_dict['test_bbl'] = testset_bbl\n",
    "    output_dict['cat2id'] = CAT2ID\n",
    "\n",
    "    annid2attid = {}\n",
    "    attanns = {}\n",
    "    for att_id, ann_id in cocottributes['patch_id_to_ann_id'].items():\n",
    "        annid2attid[ann_id] = att_id\n",
    "    for ann in coco_data['annotations']:\n",
    "        if ann['id'] in annid2attid:\n",
    "            attanns[annid2attid[ann['id']]] = ann\n",
    "        \n",
    "    output_dict['annotations'] = attanns\n",
    "\n",
    "    \n",
    "    joblib.dump(output_dict, OUTPUT_PATH.format(intra_type, inter_type), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of category:  29\n",
      "number of attribute:  204\n",
      "Class Size with Cat:  [(65000, 'person'), (7666, 'car'), (5654, 'truck'), (5524, 'motorcycle'), (5428, 'sheep'), (5323, 'horse'), (5183, 'cow'), (5074, 'broccoli'), (4973, 'elephant'), (4969, 'dog'), (4889, 'giraffe'), (4851, 'pizza'), (4808, 'bus'), (4788, 'zebra'), (4773, 'cat'), (4701, 'cake'), (4649, 'banana'), (4578, 'bird'), (4337, 'train'), (4164, 'donut'), (3949, 'boat'), (3648, 'airplane'), (3636, 'sandwich'), (3368, 'bicycle'), (3305, 'orange'), (3043, 'carrot'), (2821, 'apple'), (1966, 'hot dog'), (1358, 'bear')]\n",
      "Data Size After Init is 188426\n",
      "Data Size After Init is 188426\n",
      "===== Processing: 0.0 =====\n",
      "===== Processing: 0.034482758620689655 =====\n",
      "===== Processing: 0.06896551724137931 =====\n",
      "===== Processing: 0.10344827586206896 =====\n",
      "===== Processing: 0.13793103448275862 =====\n",
      "===== Processing: 0.1724137931034483 =====\n",
      "===== Processing: 0.20689655172413793 =====\n",
      "===== Processing: 0.2413793103448276 =====\n",
      "===== Processing: 0.27586206896551724 =====\n",
      "===== Processing: 0.3103448275862069 =====\n",
      "===== Processing: 0.3448275862068966 =====\n",
      "===== Processing: 0.3793103448275862 =====\n",
      "===== Processing: 0.41379310344827586 =====\n",
      "===== Processing: 0.4482758620689655 =====\n",
      "===== Processing: 0.4827586206896552 =====\n",
      "===== Processing: 0.5172413793103449 =====\n",
      "===== Processing: 0.5517241379310345 =====\n",
      "===== Processing: 0.5862068965517241 =====\n",
      "===== Processing: 0.6206896551724138 =====\n",
      "===== Processing: 0.6551724137931034 =====\n",
      "===== Processing: 0.6896551724137931 =====\n",
      "===== Processing: 0.7241379310344828 =====\n",
      "===== Processing: 0.7586206896551724 =====\n",
      "===== Processing: 0.7931034482758621 =====\n",
      "===== Processing: 0.8275862068965517 =====\n",
      "===== Processing: 0.8620689655172413 =====\n",
      "===== Processing: 0.896551724137931 =====\n",
      "===== Processing: 0.9310344827586207 =====\n",
      "===== Processing: 0.9655172413793104 =====\n",
      "Data Size After Test-BBL is 185526\n",
      "Data Size After Test-BBL is 185526\n",
      "Data Size After Test-BL is 182626\n",
      "Data Size After Test-BL is 182626\n",
      "Max/min class sizes are 64800 / 1158\n",
      "Num of class is 29\n",
      "Data Size After Selection is 33582\n",
      "Data Size After Selection is 33582\n",
      "Data Size After Selection is 32132\n",
      "Train size is : 32132\n",
      "Val size is : 1450\n",
      "Test-LT size is : 0\n",
      "Test-BL size is : 2900\n",
      "Test-BBL size is : 2900\n"
     ]
    }
   ],
   "source": [
    "intra_type = 'lt'\n",
    "inter_type = 'bl'\n",
    "TEST_SIZE = 100\n",
    "VAL_SIZE = 50\n",
    "\n",
    "# load raw annotations\n",
    "cocottributes = joblib.load(COCO_ATTRIBUTE_PATH)\n",
    "coco_data = load_coco()\n",
    "\n",
    "# get statistics\n",
    "att_statistics, cat_statistics, key2cats = get_statistics(cocottributes, cocottributes['ann_vecs'], coco_data)\n",
    "# generate splits\n",
    "trainset, valset, testset_lt, testset_bl, testset_bbl, CAT2ID = generate_train_val_test(cocottributes, key2cats, TEST_SIZE=TEST_SIZE, VAL_SIZE=VAL_SIZE, intra_type=intra_type, inter_type=inter_type)\n",
    "save_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of category:  29\n",
      "number of attribute:  204\n",
      "Class Size with Cat:  [(65000, 'person'), (7666, 'car'), (5654, 'truck'), (5524, 'motorcycle'), (5428, 'sheep'), (5323, 'horse'), (5183, 'cow'), (5074, 'broccoli'), (4973, 'elephant'), (4969, 'dog'), (4889, 'giraffe'), (4851, 'pizza'), (4808, 'bus'), (4788, 'zebra'), (4773, 'cat'), (4701, 'cake'), (4649, 'banana'), (4578, 'bird'), (4337, 'train'), (4164, 'donut'), (3949, 'boat'), (3648, 'airplane'), (3636, 'sandwich'), (3368, 'bicycle'), (3305, 'orange'), (3043, 'carrot'), (2821, 'apple'), (1966, 'hot dog'), (1358, 'bear')]\n",
      "Data Size After Init is 188426\n",
      "Data Size After Init is 188426\n",
      "===== Processing: 0.0 =====\n",
      "===== Processing: 0.034482758620689655 =====\n",
      "===== Processing: 0.06896551724137931 =====\n",
      "===== Processing: 0.10344827586206896 =====\n",
      "===== Processing: 0.13793103448275862 =====\n",
      "===== Processing: 0.1724137931034483 =====\n",
      "===== Processing: 0.20689655172413793 =====\n",
      "===== Processing: 0.2413793103448276 =====\n",
      "===== Processing: 0.27586206896551724 =====\n",
      "===== Processing: 0.3103448275862069 =====\n",
      "===== Processing: 0.3448275862068966 =====\n",
      "===== Processing: 0.3793103448275862 =====\n",
      "===== Processing: 0.41379310344827586 =====\n",
      "===== Processing: 0.4482758620689655 =====\n",
      "===== Processing: 0.4827586206896552 =====\n",
      "===== Processing: 0.5172413793103449 =====\n",
      "===== Processing: 0.5517241379310345 =====\n",
      "===== Processing: 0.5862068965517241 =====\n",
      "===== Processing: 0.6206896551724138 =====\n",
      "===== Processing: 0.6551724137931034 =====\n",
      "===== Processing: 0.6896551724137931 =====\n",
      "===== Processing: 0.7241379310344828 =====\n",
      "===== Processing: 0.7586206896551724 =====\n",
      "===== Processing: 0.7931034482758621 =====\n",
      "===== Processing: 0.8275862068965517 =====\n",
      "===== Processing: 0.8620689655172413 =====\n",
      "===== Processing: 0.896551724137931 =====\n",
      "===== Processing: 0.9310344827586207 =====\n",
      "===== Processing: 0.9655172413793104 =====\n",
      "Data Size After Test-BBL is 182626\n",
      "Data Size After Test-BBL is 182626\n",
      "Data Size After Test-BL is 176826\n",
      "Data Size After Test-BL is 176826\n",
      "Max/min class sizes are 64600 / 958\n",
      "Num of class is 29\n",
      "Data Size After Selection is 153273\n",
      "Data Size After Selection is 147473\n",
      "Data Size After Selection is 144573\n",
      "Train size is : 144573\n",
      "Val size is : 2900\n",
      "Test-LT size is : 5800\n",
      "Test-BL size is : 5800\n",
      "Test-BBL size is : 5800\n"
     ]
    }
   ],
   "source": [
    "intra_type = 'lt'\n",
    "inter_type = 'lt'\n",
    "TEST_SIZE = 200\n",
    "VAL_SIZE = 100\n",
    "\n",
    "# load raw annotations\n",
    "cocottributes = joblib.load(COCO_ATTRIBUTE_PATH)\n",
    "coco_data = load_coco()\n",
    "\n",
    "# get statistics\n",
    "att_statistics, cat_statistics, key2cats = get_statistics(cocottributes, cocottributes['ann_vecs'], coco_data)\n",
    "# generate splits\n",
    "trainset, valset, testset_lt, testset_bl, testset_bbl, CAT2ID = generate_train_val_test(cocottributes, key2cats, TEST_SIZE=TEST_SIZE, VAL_SIZE=VAL_SIZE, intra_type=intra_type, inter_type=inter_type)\n",
    "save_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
